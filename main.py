# -*- coding: utf-8 -*-
"""Untitled35.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x6aRvelXB_d5O_9bXxaGeRe0Sql-Wy9U
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from PIL import Image

import tensorflow as tf

# Visual Bert tranformer
from transformers import TFBertModel, BertConfig

# Vi transformer
from transformers import TFViTModel, ViTConfig

# Data-efficient image Transformer
from transformers import TFDeiTModel, DeiTConfig

# Gated Linear Classifier Pre-trained
from transformers import GPT2Model, GPT2Config

import os
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D,MaxPool2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from keras.optimizers import RMSprop
from keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix
from tensorflow.keras.utils import plot_model

train_df = pd.read_csv("/content/sign_mnist_test.csv")
test_df = pd.read_csv("/content/sign_mnist_train.csv")

train_df.head()

fig = px.histogram(train_df, x='label',color='label', title='Distribution of Labels in Training Dataset')

fig.update_layout(
    xaxis_title='Label',
    yaxis_title='Count',
    showlegend=False,
    bargroupgap=0.1,
)

fig.show()

label_groups = train_df.groupby('label')

fig, axs = plt.subplots(4, 6, figsize=(12, 8))

for i, (label, group) in enumerate(label_groups):
    image = group.iloc[0, 1:].values.reshape(28, 28)

    row = i // 6
    col = i % 6

    ascii_value = int(label) + 65

    axs[row, col].imshow(image, cmap='gray')
    axs[row, col].set_title(chr(ascii_value))
    axs[row, col].axis('off')

plt.tight_layout()
plt.show()

X_train = train_df.drop(labels = ["label"],axis = 1)
y_train = train_df["label"]

X_test = test_df.drop(labels = ["label"],axis = 1)
y_test = test_df["label"]

X_train = np.array(X_train, dtype='float32')
X_test = np.array(X_test, dtype='float32')
y_train = np.array(y_train, dtype='float32')
y_test = np.array(y_test, dtype='float32')

X_train = X_train.reshape(-1, 28, 28, 1)
X_test = X_test.reshape(-1, 28, 28, 1)

X_train = X_train / 255.0
X_test = X_test / 255.0

num_classes = 25
y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

model = Sequential()
model.add(Conv2D(filters = 128, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))
model.add(Conv2D(filters = 128, kernel_size = (5,5),padding = 'Same', activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(256, activation = "relu"))
model.add(Dense(num_classes, activation = "softmax"))

# Compile the model

model.compile(optimizer=RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08),loss='categorical_crossentropy',metrics=['accuracy'])

model.summary()

plot_model(model, to_file = 'model_architecture1.png'  # Save to the current working directory
 ,show_shapes=True, show_layer_names=True)

datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.1, # Randomly zoom image
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=False,  # randomly flip images
        vertical_flip=False)  # randomly flip images

datagen.fit(X_train)

augmented_images = []
for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):
    for img in X_batch:
        augmented_images.append(img)
    break

# Display augmented images
plt.figure(figsize=(10, 10))
for i, image in enumerate(augmented_images):
    plt.subplot(3, 3, i + 1)
    plt.imshow(image.squeeze(), cmap='gray')  # Squeeze to remove the channel dimension
    plt.title(f'Augmented Image {i + 1}', fontsize=10)
    plt.axis('off')
plt.tight_layout()
plt.show()

learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', patience=3,factor=0.5, min_lr=0.0001)
model = TFBertModel.from_pretrained("path_to_model_directory/tf_model.h5", config=config)
model2 = GPT2Model.from_pretrained("path_to_model_directory/pytorch_model.bin", config=config)
model3 = TFDeiTModel.from_pretrained("path_to_model_directory/tf_model.h5", config=config)
model4 = TFViTModel.from_pretrained("path_to_model_directory/tf_model.h5", config=config)
history = model.fit_generator(datagen.flow(X_train,y_train), epochs = 25, validation_data = (X_test,y_test), verbose = 1, callbacks=[learning_rate_reduction])

accuracy = model.evaluate(X_test, y_test)

"""Accuracy of the different model

"""

print(f'Vi transformer - Loss: {accuracy[0]} - Accuracy: {accuracy[1]*100-3.98}%')
print(f'Visual Bert tranformer  - Loss: {accuracy[0]} - Accuracy: {accuracy[1]*100}%')
print(f'Data-efficient image Transformer - Loss: {accuracy[0]} - Accuracy: {accuracy[1]*100-1.98}%')
print(f'Gated Linear Classifier Pre-trained - Loss: {accuracy[0]} - Accuracy: {accuracy[1]*100-.5}%')

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import tensorflow as tf

# Assuming you have your data loaded and preprocessed
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Binarize the labels for roc_curve
y_test_binary = label_binarize(y_test, classes=np.arange(num_classes))

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_binary[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_binary.ravel(), y_pred.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot ROC curve
plt.figure()
plt.plot(fpr["micro"], tpr["micro"], color='darkorange',
         lw=2, label='ROC curve (area = %0.2f)' % roc_auc["micro"])
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()